{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 10211,
          "databundleVersionId": 111096,
          "sourceType": "competition"
        }
      ],
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Exercise: Pipelines",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()"
      ],
      "metadata": {
        "id": "CLpXupwW_nSP"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, you will use **pipelines** to improve the efficiency of your machine learning code.\n",
        "\n",
        "# Setup\n",
        "\n",
        "The questions below will give you feedback on your work. Run the following cell to set up the feedback system."
      ],
      "metadata": {
        "id": "1OETy7l9_nST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up code checking\n",
        "import os\n",
        "if not os.path.exists(\"../input/train.csv\"):\n",
        "    os.symlink(\"../input/home-data-for-ml-course/train.csv\", \"../input/train.csv\")\n",
        "    os.symlink(\"../input/home-data-for-ml-course/test.csv\", \"../input/test.csv\")\n",
        "from learntools.core import binder\n",
        "binder.bind(globals())\n",
        "from learntools.ml_intermediate.ex4 import *\n",
        "print(\"Setup Complete\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T00:14:53.908745Z",
          "iopub.execute_input": "2024-10-31T00:14:53.90948Z",
          "iopub.status.idle": "2024-10-31T00:14:53.93845Z",
          "shell.execute_reply.started": "2024-10-31T00:14:53.909438Z",
          "shell.execute_reply": "2024-10-31T00:14:53.937448Z"
        },
        "trusted": true,
        "id": "9y847HZX_nST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Read the data\n",
        "X_full = pd.read_csv('../input/train.csv', index_col='Id')\n",
        "X_test_full = pd.read_csv('../input/test.csv', index_col='Id')\n",
        "\n",
        "# Remove rows with missing target, separate target from predictors\n",
        "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
        "# Above line is removing any rows from X_full where the SalePrice column has missing (NaN) values.\n",
        "# axis=0: Specifies that rows (rather than columns) should be dropped.\n",
        "# subset=['SalePrice']: Only considers the SalePrice column when checking for missing values, ignoring NaNs in other columns.\n",
        "# inplace=True: Directly modifies X_full by dropping the rows instead of creating a new DataFrame.\n",
        "\n",
        "y = X_full.SalePrice\n",
        "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
        "\n",
        "# Break off validation set from training data\n",
        "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y,\n",
        "                                                                train_size=0.8, test_size=0.2,\n",
        "                                                                random_state=0)\n",
        "\n",
        "# \"Cardinality\" means the number of unique values in a column\n",
        "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
        "categorical_cols = [cname for cname in X_train_full.columns if\n",
        "                    X_train_full[cname].nunique() < 10 and\n",
        "                    X_train_full[cname].dtype == \"object\"]\n",
        "\n",
        "# Select numerical columns\n",
        "numerical_cols = [cname for cname in X_train_full.columns if\n",
        "                X_train_full[cname].dtype in ['int64', 'float64']]\n",
        "\n",
        "# Keep selected columns only\n",
        "my_cols = categorical_cols + numerical_cols\n",
        "X_train = X_train_full[my_cols].copy()\n",
        "X_valid = X_valid_full[my_cols].copy()\n",
        "X_test = X_test_full[my_cols].copy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T00:14:57.123919Z",
          "iopub.execute_input": "2024-10-31T00:14:57.124563Z",
          "iopub.status.idle": "2024-10-31T00:14:57.229855Z",
          "shell.execute_reply.started": "2024-10-31T00:14:57.124521Z",
          "shell.execute_reply": "2024-10-31T00:14:57.228743Z"
        },
        "trusted": true,
        "id": "auidJZUc_nSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T00:15:00.073054Z",
          "iopub.execute_input": "2024-10-31T00:15:00.073508Z",
          "iopub.status.idle": "2024-10-31T00:15:00.099666Z",
          "shell.execute_reply.started": "2024-10-31T00:15:00.073463Z",
          "shell.execute_reply": "2024-10-31T00:15:00.098548Z"
        },
        "trusted": true,
        "id": "UTJ-JspJ_nSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next code cell uses code from the tutorial to preprocess the data and train a model.  Run this code without changes."
      ],
      "metadata": {
        "id": "K_79PV8e_nSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Preprocessing for numerical data\n",
        "numerical_transformer = SimpleImputer(strategy='constant')\n",
        "\n",
        "# Preprocessing for categorical data\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Bundle preprocessing for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Define model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
        "\n",
        "# Bundle preprocessing and modeling code in a pipeline\n",
        "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('model', model)\n",
        "                     ])\n",
        "\n",
        "# Preprocessing of training data, fit model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Preprocessing of validation data, get predictions\n",
        "preds = clf.predict(X_valid)\n",
        "\n",
        "print('MAE:', mean_absolute_error(y_valid, preds))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T00:15:02.227495Z",
          "iopub.execute_input": "2024-10-31T00:15:02.228327Z",
          "iopub.status.idle": "2024-10-31T00:15:04.746525Z",
          "shell.execute_reply.started": "2024-10-31T00:15:02.228268Z",
          "shell.execute_reply": "2024-10-31T00:15:04.745401Z"
        },
        "trusted": true,
        "id": "234RhedR_nSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code yields a value around 17862 for the mean absolute error (MAE).  In the next step, you will amend the code to do better.\n",
        "\n",
        "# Step 1: Improve the performance\n",
        "\n",
        "### Part A\n",
        "\n",
        "Now, it's your turn!  In the code cell below, define your own preprocessing steps and random forest model.  Fill in values for the following variables:\n",
        "- `numerical_transformer`\n",
        "- `categorical_transformer`\n",
        "- `model`\n",
        "\n",
        "To pass this part of the exercise, you need only define valid preprocessing steps and a random forest model."
      ],
      "metadata": {
        "id": "FyIRwUCb_nSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Preprocessing for numerical data\n",
        "numerical_transformer = KNNImputer(n_neighbors=10)  # using default settings, but you can adjust n_neighbors based on your data\n",
        "\n",
        "# Preprocessing for categorical data\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant')),  # updated to fill categorical with 'missing'\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Bundle preprocessing for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Define model\n",
        "model = RandomForestRegressor(n_estimators = 100, random_state=0)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T00:15:08.227907Z",
          "iopub.execute_input": "2024-10-31T00:15:08.228283Z",
          "iopub.status.idle": "2024-10-31T00:15:08.241083Z",
          "shell.execute_reply.started": "2024-10-31T00:15:08.228247Z",
          "shell.execute_reply": "2024-10-31T00:15:08.239921Z"
        },
        "trusted": true,
        "id": "MBAqdJq8_nSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Below, we advance the code using:\n",
        "1-Use KNN as a more advanced imputer\n",
        "2-Hyperparameter tuning for the Regressor and for the numerical-Imputer\n",
        "3-Set those optimal parameters as the model's parameters**"
      ],
      "metadata": {
        "id": "z_T5dQxb_nSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Preprocessing for numerical data\n",
        "numerical_transformer = KNNImputer()  # No specific n_neighbors set here\n",
        "\n",
        "# Preprocessing for categorical data\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Bundle preprocessing for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Define model\n",
        "model = RandomForestRegressor(random_state=0)\n",
        "\n",
        "# Bundle preprocessing and modeling code in a pipeline\n",
        "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                              ('model', model)\n",
        "                             ])\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_dist = {\n",
        "    'preprocessor__num__n_neighbors': [5, 10, 20],\n",
        "    'model__n_estimators': [100, 200],\n",
        "    'model__max_depth': [None, 10, 30],\n",
        "    'model__min_samples_split': [2, 5],\n",
        "    'model__min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "search = RandomizedSearchCV(my_pipeline, param_dist, n_iter=30, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "# Update preprocessor and model with the best parameters found\n",
        "best_params = search.best_params_\n",
        "preprocessor.set_params(num__n_neighbors=best_params['preprocessor__num__n_neighbors'])\n",
        "model.set_params(\n",
        "    n_estimators=best_params['model__n_estimators'],\n",
        "    max_depth=best_params['model__max_depth'],\n",
        "    min_samples_split=best_params['model__min_samples_split'],\n",
        "    min_samples_leaf=best_params['model__min_samples_leaf']\n",
        ")\n",
        "\n",
        "# Optionally print the best parameters and/or re-check\n",
        "print('Best parameters:', best_params)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T00:20:55.064566Z",
          "iopub.execute_input": "2024-10-31T00:20:55.064965Z",
          "iopub.status.idle": "2024-10-31T00:23:12.794198Z",
          "shell.execute_reply.started": "2024-10-31T00:20:55.064925Z",
          "shell.execute_reply": "2024-10-31T00:23:12.793062Z"
        },
        "trusted": true,
        "id": "NSPgOvyl_nSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B\n",
        "\n",
        "Run the code cell below without changes.\n",
        "\n",
        "To pass this step, you need to have defined a pipeline in **Part A** that achieves lower MAE than the code above.  You're encouraged to take your time here and try out many different approaches, to see how low you can get the MAE!  (_If your code does not pass, please amend the preprocessing steps and model in Part A._)"
      ],
      "metadata": {
        "id": "jHEHGCA6_nSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bundle preprocessing and modeling code in a pipeline\n",
        "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                              ('model', model)\n",
        "                             ])\n",
        "\n",
        "# Preprocessing of training data, fit model\n",
        "my_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Preprocessing of validation data, get predictions\n",
        "preds = my_pipeline.predict(X_valid)\n",
        "\n",
        "# Evaluate the model\n",
        "score = mean_absolute_error(y_valid, preds)\n",
        "print('MAE:', score)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T00:24:22.583686Z",
          "iopub.execute_input": "2024-10-31T00:24:22.584132Z",
          "iopub.status.idle": "2024-10-31T00:24:27.980032Z",
          "shell.execute_reply.started": "2024-10-31T00:24:22.584091Z",
          "shell.execute_reply": "2024-10-31T00:24:27.977441Z"
        },
        "trusted": true,
        "id": "GAjrLLFb_nSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Generate test predictions\n",
        "\n",
        "Now, you'll use your trained model to generate predictions with the test data."
      ],
      "metadata": {
        "id": "0buxkWL5_nSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing of test data, fit model\n",
        "preds_test = my_pipeline.predict(X_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T00:26:32.689509Z",
          "iopub.execute_input": "2024-10-31T00:26:32.689911Z",
          "iopub.status.idle": "2024-10-31T00:26:32.862837Z",
          "shell.execute_reply.started": "2024-10-31T00:26:32.68987Z",
          "shell.execute_reply": "2024-10-31T00:26:32.861629Z"
        },
        "trusted": true,
        "id": "vjGgOZ-U_nSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the next code cell without changes to save your results to a CSV file that can be submitted directly to the competition."
      ],
      "metadata": {
        "id": "K5J9zBB9_nSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save test predictions to file\n",
        "output = pd.DataFrame({'Id': X_test.index,\n",
        "                       'SalePrice': preds_test})\n",
        "output.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-31T00:26:46.690617Z",
          "iopub.execute_input": "2024-10-31T00:26:46.691321Z",
          "iopub.status.idle": "2024-10-31T00:26:46.700638Z",
          "shell.execute_reply.started": "2024-10-31T00:26:46.691265Z",
          "shell.execute_reply": "2024-10-31T00:26:46.69959Z"
        },
        "trusted": true,
        "id": "pNfLJNTC_nSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/intermediate-machine-learning/discussion) to chat with other learners.*"
      ],
      "metadata": {
        "id": "ijxQ4zAN_nSW"
      }
    }
  ]
}